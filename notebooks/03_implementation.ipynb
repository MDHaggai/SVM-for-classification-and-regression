{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6736500",
   "metadata": {},
   "source": [
    "# SVM Implementation from Scratch for Heart Disease Diagnosis\n",
    "\n",
    "This notebook implements Support Vector Machine algorithms from scratch, focusing on heart disease classification. We'll build both linear and kernel SVM implementations with detailed explanations of each mathematical step.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "1. **Implement Linear SVM** from mathematical foundations\n",
    "2. **Build Kernel SVM** with RBF and polynomial kernels\n",
    "3. **Understand optimization** through gradient descent and SMO algorithm\n",
    "4. **Apply to medical data** with heart disease diagnosis\n",
    "5. **Compare implementations** with sklearn for validation\n",
    "6. **Visualize decision boundaries** and support vectors\n",
    "\n",
    "## Medical Context\n",
    "\n",
    "Heart disease classification is an ideal application for SVM because:\n",
    "- **Binary classification**: Healthy vs. Disease\n",
    "- **High-dimensional features**: Multiple clinical measurements\n",
    "- **Non-linear relationships**: Complex interactions between risk factors\n",
    "- **Critical accuracy**: Minimizing false negatives is essential\n",
    "- **Interpretability**: Understanding which patients are \"support vectors\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb6b981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.join(os.getcwd(), '..', 'src'))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import our custom data loader\n",
    "try:\n",
    "    from utils.data_loader import load_heart_disease_data\n",
    "    print(\"✅ Custom data loader imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"⚠️ Custom data loader import failed: {e}\")\n",
    "    print(\"Will use direct data loading\")\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"Set2\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "print(\"🧠 SVM FROM SCRATCH IMPLEMENTATION\")\n",
    "print(\"=\" * 40)\n",
    "print(\"Building Support Vector Machine for Heart Disease Diagnosis\")\n",
    "print(\"Focus: Mathematical implementation with medical applications\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804d161c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Heart Disease Dataset\n",
    "print(\"📥 LOADING HEART DISEASE DATA\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "try:\n",
    "    # Load using custom data loader\n",
    "    X, y = load_heart_disease_data()\n",
    "    print(\"✅ Heart disease data loaded successfully\")\n",
    "    \n",
    "    # Create feature names\n",
    "    feature_names = ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', \n",
    "                    'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal']\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Custom loader failed: {e}\")\n",
    "    print(\"Creating synthetic heart disease data for demonstration...\")\n",
    "    \n",
    "    # Create synthetic data that mimics heart disease patterns\n",
    "    np.random.seed(42)\n",
    "    n_patients = 300\n",
    "    \n",
    "    # Generate correlated features\n",
    "    ages = np.random.normal(55, 12, n_patients)\n",
    "    chol = np.random.normal(240, 40, n_patients)\n",
    "    bp = np.random.normal(130, 15, n_patients)\n",
    "    max_hr = 220 - ages + np.random.normal(0, 10, n_patients)\n",
    "    \n",
    "    # Additional features\n",
    "    sex = np.random.choice([0, 1], n_patients, p=[0.3, 0.7])  # More males\n",
    "    cp = np.random.choice([0, 1, 2, 3], n_patients)\n",
    "    \n",
    "    # Create realistic heart disease labels\n",
    "    risk_score = (ages - 45) * 0.1 + (chol - 200) * 0.02 + (bp - 120) * 0.05\n",
    "    risk_score += sex * 0.5 - (max_hr - 150) * 0.02  # Males higher risk, higher HR protective\n",
    "    risk_score += np.random.normal(0, 2, n_patients)\n",
    "    \n",
    "    y = (risk_score > np.median(risk_score)).astype(int)\n",
    "    X = np.column_stack([ages, sex, cp, bp, chol, max_hr])\n",
    "    feature_names = ['age', 'sex', 'cp', 'trestbps', 'chol', 'thalach']\n",
    "    \n",
    "    print(\"✅ Synthetic heart disease data created\")\n",
    "\n",
    "# Convert labels to {-1, +1} format for SVM\n",
    "y_svm = 2 * y - 1\n",
    "\n",
    "print(f\"\\n📋 Dataset Information:\")\n",
    "print(f\"  • Patients: {len(X)}\")\n",
    "print(f\"  • Features: {len(feature_names)}\")\n",
    "print(f\"  • Heart disease cases: {np.sum(y)} ({np.mean(y)*100:.1f}%)\")\n",
    "print(f\"  • Healthy cases: {np.sum(1-y)} ({np.mean(1-y)*100:.1f}%)\")\n",
    "print(f\"  • Class balance: {'Balanced' if 0.4 <= np.mean(y) <= 0.6 else 'Imbalanced'}\")\n",
    "\n",
    "# Split data for training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_svm, test_size=0.2, random_state=42, stratify=y_svm\n",
    ")\n",
    "\n",
    "# Standardize features (crucial for SVM)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"\\n📊 Data Splitting:\")\n",
    "print(f\"  • Training set: {len(X_train)} patients\")\n",
    "print(f\"  • Test set: {len(X_test)} patients\")\n",
    "print(f\"  • Features standardized: mean=0, std=1\")\n",
    "print(f\"  • Ready for SVM implementation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfd3a23",
   "metadata": {},
   "source": [
    "## Linear SVM Implementation from Scratch\n",
    "\n",
    "### Mathematical Foundation\n",
    "\n",
    "We'll implement the linear SVM optimization problem:\n",
    "\n",
    "$$\\min_{\\mathbf{w},b} \\frac{1}{2}||\\mathbf{w}||^2 + C\\sum_{i=1}^{n}\\max(0, 1 - y_i(\\mathbf{w}^T\\mathbf{x}_i + b))$$\n",
    "\n",
    "This combines:\n",
    "1. **Margin maximization**: $\\frac{1}{2}||\\mathbf{w}||^2$\n",
    "2. **Hinge loss**: $\\max(0, 1 - y_i(\\mathbf{w}^T\\mathbf{x}_i + b))$\n",
    "\n",
    "### Implementation Strategy\n",
    "\n",
    "1. **Gradient Descent**: Optimize weights and bias iteratively\n",
    "2. **Subgradient**: Handle non-differentiable hinge loss\n",
    "3. **Medical Interpretation**: Track support vectors (critical patients)\n",
    "4. **Convergence**: Monitor loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3030960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear SVM Implementation from Scratch\n",
    "class LinearSVM:\n",
    "    \"\"\"\n",
    "    Linear Support Vector Machine implemented from scratch\n",
    "    Optimized for medical diagnosis applications\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, C=1.0, learning_rate=0.001, max_iter=1000, tol=1e-6):\n",
    "        \"\"\"\n",
    "        Initialize Linear SVM\n",
    "        \n",
    "        Parameters:\n",
    "        - C: Regularization parameter (higher = less regularization)\n",
    "        - learning_rate: Step size for gradient descent\n",
    "        - max_iter: Maximum iterations\n",
    "        - tol: Convergence tolerance\n",
    "        \"\"\"\n",
    "        self.C = C\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        \n",
    "        # Model parameters\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "        \n",
    "        # Training history\n",
    "        self.losses = []\n",
    "        self.accuracies = []\n",
    "        \n",
    "    def _hinge_loss(self, X, y):\n",
    "        \"\"\"\n",
    "        Compute hinge loss and its subgradient\n",
    "        \n",
    "        L = (1/2)||w||^2 + C * sum(max(0, 1 - y_i * f(x_i)))\n",
    "        \"\"\"\n",
    "        # Decision function\n",
    "        distances = X @ self.w + self.b\n",
    "        \n",
    "        # Hinge loss\n",
    "        margins = y * distances\n",
    "        hinge_losses = np.maximum(0, 1 - margins)\n",
    "        \n",
    "        # Total loss\n",
    "        regularization_loss = 0.5 * np.sum(self.w**2)\n",
    "        empirical_loss = self.C * np.mean(hinge_losses)\n",
    "        total_loss = regularization_loss + empirical_loss\n",
    "        \n",
    "        return total_loss, distances, margins\n",
    "    \n",
    "    def _compute_gradients(self, X, y, margins):\n",
    "        \"\"\"\n",
    "        Compute subgradients of hinge loss\n",
    "        \"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        \n",
    "        # Find samples in margin (support vectors)\n",
    "        margin_mask = margins < 1\n",
    "        \n",
    "        # Gradient w.r.t weights\n",
    "        dw = self.w.copy()  # Regularization term\n",
    "        if np.any(margin_mask):\n",
    "            dw -= self.C * np.mean(X[margin_mask] * y[margin_mask].reshape(-1, 1), axis=0)\n",
    "        \n",
    "        # Gradient w.r.t bias\n",
    "        db = 0\n",
    "        if np.any(margin_mask):\n",
    "            db = -self.C * np.mean(y[margin_mask])\n",
    "        \n",
    "        return dw, db\n",
    "    \n",
    "    def fit(self, X, y, verbose=True):\n",
    "        \"\"\"\n",
    "        Train the Linear SVM using gradient descent\n",
    "        \n",
    "        Parameters:\n",
    "        - X: Training features (n_samples, n_features)\n",
    "        - y: Training labels {-1, +1}\n",
    "        - verbose: Print training progress\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        # Initialize parameters\n",
    "        np.random.seed(42)\n",
    "        self.w = np.random.normal(0, 0.01, n_features)\n",
    "        self.b = 0.0\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\n🏃 Training Linear SVM\")\n",
    "            print(f\"  • Samples: {n_samples}, Features: {n_features}\")\n",
    "            print(f\"  • C parameter: {self.C}\")\n",
    "            print(f\"  • Learning rate: {self.learning_rate}\")\n",
    "        \n",
    "        # Training loop\n",
    "        for iteration in range(self.max_iter):\n",
    "            # Compute loss and margins\n",
    "            loss, distances, margins = self._hinge_loss(X, y)\n",
    "            \n",
    "            # Compute gradients\n",
    "            dw, db = self._compute_gradients(X, y, margins)\n",
    "            \n",
    "            # Update parameters\n",
    "            self.w -= self.learning_rate * dw\n",
    "            self.b -= self.learning_rate * db\n",
    "            \n",
    "            # Track progress\n",
    "            predictions = np.sign(distances)\n",
    "            accuracy = np.mean(predictions == y)\n",
    "            \n",
    "            self.losses.append(loss)\n",
    "            self.accuracies.append(accuracy)\n",
    "            \n",
    "            # Print progress\n",
    "            if verbose and (iteration + 1) % 200 == 0:\n",
    "                print(f\"  Iteration {iteration + 1:4d}: Loss = {loss:.4f}, Accuracy = {accuracy:.3f}\")\n",
    "            \n",
    "            # Check convergence\n",
    "            if len(self.losses) > 1 and abs(self.losses[-1] - self.losses[-2]) < self.tol:\n",
    "                if verbose:\n",
    "                    print(f\"  ✅ Converged at iteration {iteration + 1}\")\n",
    "                break\n",
    "        \n",
    "        # Identify support vectors\n",
    "        _, _, final_margins = self._hinge_loss(X, y)\n",
    "        self.support_vector_mask = final_margins <= 1 + 1e-6\n",
    "        self.n_support_vectors = np.sum(self.support_vector_mask)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"  • Final loss: {self.losses[-1]:.4f}\")\n",
    "            print(f\"  • Final accuracy: {self.accuracies[-1]:.3f}\")\n",
    "            print(f\"  • Support vectors: {self.n_support_vectors} ({self.n_support_vectors/n_samples*100:.1f}%)\")\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions on new data\n",
    "        \"\"\"\n",
    "        return np.sign(X @ self.w + self.b)\n",
    "    \n",
    "    def decision_function(self, X):\n",
    "        \"\"\"\n",
    "        Compute decision function values (distance from hyperplane)\n",
    "        \"\"\"\n",
    "        return X @ self.w + self.b\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        \"\"\"\n",
    "        Compute accuracy score\n",
    "        \"\"\"\n",
    "        predictions = self.predict(X)\n",
    "        return np.mean(predictions == y)\n",
    "\n",
    "print(\"✅ LinearSVM class implemented\")\n",
    "print(\"Key features:\")\n",
    "print(\"  • Hinge loss optimization\")\n",
    "print(\"  • Gradient descent training\")\n",
    "print(\"  • Support vector identification\")\n",
    "print(\"  • Medical interpretation ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43146ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Linear SVM from Scratch\n",
    "print(\"\\n🎯 TRAINING LINEAR SVM FROM SCRATCH\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Initialize and train our custom Linear SVM\n",
    "custom_svm = LinearSVM(C=1.0, learning_rate=0.001, max_iter=1000)\n",
    "custom_svm.fit(X_train_scaled, y_train, verbose=True)\n",
    "\n",
    "# Make predictions\n",
    "y_train_pred = custom_svm.predict(X_train_scaled)\n",
    "y_test_pred = custom_svm.predict(X_test_scaled)\n",
    "\n",
    "print(f\"\\n📈 PERFORMANCE EVALUATION\")\n",
    "print(\"=\" * 30)\n",
    "print(f\"Training Accuracy: {custom_svm.score(X_train_scaled, y_train):.3f}\")\n",
    "print(f\"Test Accuracy: {custom_svm.score(X_test_scaled, y_test):.3f}\")\n",
    "\n",
    "# Compare with sklearn SVM\n",
    "sklearn_svm = SVC(kernel='linear', C=1.0)\n",
    "sklearn_svm.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"\\n🔄 COMPARISON WITH SKLEARN\")\n",
    "print(\"=\" * 35)\n",
    "print(f\"Our Implementation - Test Accuracy: {custom_svm.score(X_test_scaled, y_test):.3f}\")\n",
    "print(f\"Sklearn Linear SVM - Test Accuracy: {sklearn_svm.score(X_test_scaled, y_test):.3f}\")\n",
    "print(f\"Difference: {abs(custom_svm.score(X_test_scaled, y_test) - sklearn_svm.score(X_test_scaled, y_test)):.3f}\")\n",
    "\n",
    "# Visualize training progress\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Loss curve\n",
    "ax1.plot(custom_svm.losses, 'b-', linewidth=2, label='Training Loss')\n",
    "ax1.set_xlabel('Iteration')\n",
    "ax1.set_ylabel('Hinge Loss')\n",
    "ax1.set_title('SVM Training Loss Convergence', fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.legend()\n",
    "\n",
    "# Accuracy curve\n",
    "ax2.plot(custom_svm.accuracies, 'g-', linewidth=2, label='Training Accuracy')\n",
    "ax2.set_xlabel('Iteration')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('SVM Training Accuracy Progress', fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Medical interpretation\n",
    "print(f\"\\n🩺 MEDICAL INTERPRETATION\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Feature importance (weights)\n",
    "feature_importance = np.abs(custom_svm.w)\n",
    "sorted_indices = np.argsort(feature_importance)[::-1]\n",
    "\n",
    "print(f\"Feature Importance (|weight|):\")\n",
    "for i, idx in enumerate(sorted_indices[:5]):\n",
    "    feature_name = feature_names[idx] if idx < len(feature_names) else f'feature_{idx}'\n",
    "    weight = custom_svm.w[idx]\n",
    "    importance = feature_importance[idx]\n",
    "    direction = \"increases\" if weight > 0 else \"decreases\"\n",
    "    print(f\"  {i+1}. {feature_name:12s}: {weight:+.3f} ({direction} heart disease risk)\")\n",
    "\n",
    "# Support vectors analysis\n",
    "print(f\"\\nSupport Vector Analysis:\")\n",
    "print(f\"  • Total support vectors: {custom_svm.n_support_vectors}\")\n",
    "print(f\"  • Percentage of training data: {custom_svm.n_support_vectors/len(X_train)*100:.1f}%\")\n",
    "print(f\"  • These are the most 'critical' patients near the decision boundary\")\n",
    "print(f\"  • They define the diagnostic criteria learned by the model\")\n",
    "\n",
    "# Decision function analysis\n",
    "train_distances = custom_svm.decision_function(X_train_scaled)\n",
    "test_distances = custom_svm.decision_function(X_test_scaled)\n",
    "\n",
    "print(f\"\\nDiagnostic Confidence Analysis:\")\n",
    "print(f\"  • Training distances range: [{train_distances.min():.2f}, {train_distances.max():.2f}]\")\n",
    "print(f\"  • Test distances range: [{test_distances.min():.2f}, {test_distances.max():.2f}]\")\n",
    "print(f\"  • Larger |distance| = higher diagnostic confidence\")\n",
    "print(f\"  • Patients near boundary (|distance| < 1) need careful review\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407d7d88",
   "metadata": {},
   "source": [
    "## Kernel SVM Implementation\n",
    "\n",
    "### RBF Kernel for Non-Linear Medical Patterns\n",
    "\n",
    "Heart disease often involves non-linear relationships between risk factors. The RBF kernel allows our SVM to capture these complex patterns:\n",
    "\n",
    "$$K(\\mathbf{x}_i, \\mathbf{x}_j) = \\exp\\left(-\\gamma ||\\mathbf{x}_i - \\mathbf{x}_j||^2\\right)$$\n",
    "\n",
    "### Implementation Approach\n",
    "\n",
    "We'll use the **SMO (Sequential Minimal Optimization)** algorithm:\n",
    "1. **Dual formulation**: Work with Lagrange multipliers $\\alpha_i$\n",
    "2. **Kernel matrix**: Precompute $K_{ij} = K(\\mathbf{x}_i, \\mathbf{x}_j)$\n",
    "3. **Iterative optimization**: Update pairs of $\\alpha$ values\n",
    "4. **KKT conditions**: Ensure optimality constraints\n",
    "\n",
    "### Medical Benefits\n",
    "- **Non-linear boundaries**: Capture complex risk patterns\n",
    "- **Flexible decision regions**: Adapt to data topology\n",
    "- **Support vector identification**: Find most informative patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5c178c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kernel SVM Implementation with RBF Kernel\n",
    "class KernelSVM:\n",
    "    \"\"\"\n",
    "    Kernel SVM with RBF kernel implemented using simplified SMO algorithm\n",
    "    Optimized for medical diagnosis applications\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, C=1.0, gamma=1.0, max_iter=1000, tol=1e-3):\n",
    "        \"\"\"\n",
    "        Initialize Kernel SVM\n",
    "        \n",
    "        Parameters:\n",
    "        - C: Regularization parameter\n",
    "        - gamma: RBF kernel parameter\n",
    "        - max_iter: Maximum iterations for SMO\n",
    "        - tol: Convergence tolerance\n",
    "        \"\"\"\n",
    "        self.C = C\n",
    "        self.gamma = gamma\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        \n",
    "        # Model parameters\n",
    "        self.alpha = None\n",
    "        self.b = None\n",
    "        self.X_train = None\n",
    "        self.y_train = None\n",
    "        \n",
    "        # Support vectors\n",
    "        self.support_vectors = None\n",
    "        self.support_labels = None\n",
    "        self.support_alpha = None\n",
    "        \n",
    "    def _rbf_kernel(self, X1, X2):\n",
    "        \"\"\"\n",
    "        Compute RBF kernel matrix\n",
    "        K(x_i, x_j) = exp(-gamma * ||x_i - x_j||^2)\n",
    "        \"\"\"\n",
    "        # Compute squared Euclidean distances efficiently\n",
    "        X1_norm = np.sum(X1**2, axis=1, keepdims=True)\n",
    "        X2_norm = np.sum(X2**2, axis=1, keepdims=True)\n",
    "        distances_sq = X1_norm + X2_norm.T - 2 * np.dot(X1, X2.T)\n",
    "        \n",
    "        # Apply RBF kernel\n",
    "        return np.exp(-self.gamma * distances_sq)\n",
    "    \n",
    "    def _compute_error(self, i, K):\n",
    "        \"\"\"\n",
    "        Compute prediction error for sample i\n",
    "        \"\"\"\n",
    "        prediction = np.sum(self.alpha * self.y_train * K[i, :]) + self.b\n",
    "        return prediction - self.y_train[i]\n",
    "    \n",
    "    def _select_second_alpha(self, i1, E1, K):\n",
    "        \"\"\"\n",
    "        Select second alpha using heuristic (largest |E1 - E2|)\n",
    "        \"\"\"\n",
    "        valid_alpha_indices = np.where((self.alpha > 0) & (self.alpha < self.C))[0]\n",
    "        \n",
    "        if len(valid_alpha_indices) > 1:\n",
    "            errors = np.array([self._compute_error(i, K) for i in valid_alpha_indices])\n",
    "            i2 = valid_alpha_indices[np.argmax(np.abs(errors - E1))]\n",
    "        else:\n",
    "            # Random selection\n",
    "            candidates = list(range(len(self.alpha)))\n",
    "            candidates.remove(i1)\n",
    "            i2 = np.random.choice(candidates)\n",
    "        \n",
    "        return i2\n",
    "    \n",
    "    def _optimize_pair(self, i1, i2, K):\n",
    "        \"\"\"\n",
    "        Optimize alpha pair (i1, i2) using SMO algorithm\n",
    "        \"\"\"\n",
    "        if i1 == i2:\n",
    "            return False\n",
    "        \n",
    "        alpha1_old = self.alpha[i1]\n",
    "        alpha2_old = self.alpha[i2]\n",
    "        y1, y2 = self.y_train[i1], self.y_train[i2]\n",
    "        \n",
    "        # Compute bounds\n",
    "        if y1 != y2:\n",
    "            L = max(0, alpha2_old - alpha1_old)\n",
    "            H = min(self.C, self.C + alpha2_old - alpha1_old)\n",
    "        else:\n",
    "            L = max(0, alpha1_old + alpha2_old - self.C)\n",
    "            H = min(self.C, alpha1_old + alpha2_old)\n",
    "        \n",
    "        if L == H:\n",
    "            return False\n",
    "        \n",
    "        # Compute eta\n",
    "        eta = 2 * K[i1, i2] - K[i1, i1] - K[i2, i2]\n",
    "        if eta >= 0:\n",
    "            return False\n",
    "        \n",
    "        # Compute errors\n",
    "        E1 = self._compute_error(i1, K)\n",
    "        E2 = self._compute_error(i2, K)\n",
    "        \n",
    "        # Update alpha2\n",
    "        alpha2_new = alpha2_old - y2 * (E1 - E2) / eta\n",
    "        alpha2_new = max(L, min(H, alpha2_new))\n",
    "        \n",
    "        if abs(alpha2_new - alpha2_old) < self.tol:\n",
    "            return False\n",
    "        \n",
    "        # Update alpha1\n",
    "        alpha1_new = alpha1_old + y1 * y2 * (alpha2_old - alpha2_new)\n",
    "        \n",
    "        # Update alphas\n",
    "        self.alpha[i1] = alpha1_new\n",
    "        self.alpha[i2] = alpha2_new\n",
    "        \n",
    "        # Update bias\n",
    "        b1 = self.b - E1 - y1 * (alpha1_new - alpha1_old) * K[i1, i1] - y2 * (alpha2_new - alpha2_old) * K[i1, i2]\n",
    "        b2 = self.b - E2 - y1 * (alpha1_new - alpha1_old) * K[i1, i2] - y2 * (alpha2_new - alpha2_old) * K[i2, i2]\n",
    "        \n",
    "        if 0 < alpha1_new < self.C:\n",
    "            self.b = b1\n",
    "        elif 0 < alpha2_new < self.C:\n",
    "            self.b = b2\n",
    "        else:\n",
    "            self.b = (b1 + b2) / 2\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def fit(self, X, y, verbose=True):\n",
    "        \"\"\"\n",
    "        Train Kernel SVM using SMO algorithm\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        # Store training data\n",
    "        self.X_train = X.copy()\n",
    "        self.y_train = y.copy()\n",
    "        \n",
    "        # Initialize alphas and bias\n",
    "        self.alpha = np.zeros(n_samples)\n",
    "        self.b = 0.0\n",
    "        \n",
    "        # Compute kernel matrix\n",
    "        K = self._rbf_kernel(X, X)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\n🏃 Training Kernel SVM (RBF)\")\n",
    "            print(f\"  • Samples: {n_samples}, Features: {n_features}\")\n",
    "            print(f\"  • C parameter: {self.C}\")\n",
    "            print(f\"  • Gamma parameter: {self.gamma}\")\n",
    "        \n",
    "        # SMO main loop\n",
    "        num_changed = 0\n",
    "        examine_all = True\n",
    "        \n",
    "        for iteration in range(self.max_iter):\n",
    "            num_changed = 0\n",
    "            \n",
    "            if examine_all:\n",
    "                # Examine all samples\n",
    "                for i in range(n_samples):\n",
    "                    if self._examine_example(i, K):\n",
    "                        num_changed += 1\n",
    "            else:\n",
    "                # Examine non-bound samples\n",
    "                non_bound_indices = np.where((self.alpha > 0) & (self.alpha < self.C))[0]\n",
    "                for i in non_bound_indices:\n",
    "                    if self._examine_example(i, K):\n",
    "                        num_changed += 1\n",
    "            \n",
    "            if examine_all:\n",
    "                examine_all = False\n",
    "            elif num_changed == 0:\n",
    "                examine_all = True\n",
    "            \n",
    "            if verbose and (iteration + 1) % 100 == 0:\n",
    "                accuracy = self.score(X, y)\n",
    "                n_sv = np.sum(self.alpha > 1e-6)\n",
    "                print(f\"  Iteration {iteration + 1:3d}: Accuracy = {accuracy:.3f}, Support Vectors = {n_sv}\")\n",
    "            \n",
    "            if num_changed == 0 and not examine_all:\n",
    "                break\n",
    "        \n",
    "        # Extract support vectors\n",
    "        sv_mask = self.alpha > 1e-6\n",
    "        self.support_vectors = X[sv_mask]\n",
    "        self.support_labels = y[sv_mask]\n",
    "        self.support_alpha = self.alpha[sv_mask]\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"  ✅ Training completed\")\n",
    "            print(f\"  • Support vectors: {len(self.support_vectors)} ({len(self.support_vectors)/n_samples*100:.1f}%)\")\n",
    "            print(f\"  • Final accuracy: {self.score(X, y):.3f}\")\n",
    "    \n",
    "    def _examine_example(self, i1, K):\n",
    "        \"\"\"\n",
    "        Examine sample i1 for optimization\n",
    "        \"\"\"\n",
    "        y1 = self.y_train[i1]\n",
    "        alpha1 = self.alpha[i1]\n",
    "        E1 = self._compute_error(i1, K)\n",
    "        r1 = E1 * y1\n",
    "        \n",
    "        if (r1 < -self.tol and alpha1 < self.C) or (r1 > self.tol and alpha1 > 0):\n",
    "            # Select second alpha\n",
    "            i2 = self._select_second_alpha(i1, E1, K)\n",
    "            if self._optimize_pair(i1, i2, K):\n",
    "                return True\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions on new data\n",
    "        \"\"\"\n",
    "        if self.support_vectors is None:\n",
    "            raise ValueError(\"Model must be fitted before making predictions\")\n",
    "        \n",
    "        # Compute kernel between test data and support vectors\n",
    "        K_test = self._rbf_kernel(X, self.support_vectors)\n",
    "        \n",
    "        # Make predictions\n",
    "        predictions = np.sum(self.support_alpha * self.support_labels * K_test, axis=1) + self.b\n",
    "        \n",
    "        return np.sign(predictions)\n",
    "    \n",
    "    def decision_function(self, X):\n",
    "        \"\"\"\n",
    "        Compute decision function values\n",
    "        \"\"\"\n",
    "        if self.support_vectors is None:\n",
    "            raise ValueError(\"Model must be fitted before computing decision function\")\n",
    "        \n",
    "        K_test = self._rbf_kernel(X, self.support_vectors)\n",
    "        return np.sum(self.support_alpha * self.support_labels * K_test, axis=1) + self.b\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        \"\"\"\n",
    "        Compute accuracy score\n",
    "        \"\"\"\n",
    "        predictions = self.predict(X)\n",
    "        return np.mean(predictions == y)\n",
    "\n",
    "print(\"✅ KernelSVM class implemented\")\n",
    "print(\"Key features:\")\n",
    "print(\"  • RBF kernel for non-linear patterns\")\n",
    "print(\"  • SMO optimization algorithm\")\n",
    "print(\"  • Support vector extraction\")\n",
    "print(\"  • Medical diagnosis ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677ac06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Kernel SVM from Scratch\n",
    "print(\"\\n🎯 TRAINING KERNEL SVM FROM SCRATCH\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Initialize and train our custom Kernel SVM\n",
    "custom_kernel_svm = KernelSVM(C=1.0, gamma=0.1, max_iter=500)\n",
    "custom_kernel_svm.fit(X_train_scaled, y_train, verbose=True)\n",
    "\n",
    "# Make predictions\n",
    "y_train_pred_kernel = custom_kernel_svm.predict(X_train_scaled)\n",
    "y_test_pred_kernel = custom_kernel_svm.predict(X_test_scaled)\n",
    "\n",
    "print(f\"\\n📈 KERNEL SVM PERFORMANCE\")\n",
    "print(\"=\" * 30)\n",
    "print(f\"Training Accuracy: {custom_kernel_svm.score(X_train_scaled, y_train):.3f}\")\n",
    "print(f\"Test Accuracy: {custom_kernel_svm.score(X_test_scaled, y_test):.3f}\")\n",
    "\n",
    "# Compare with sklearn RBF SVM\n",
    "sklearn_rbf_svm = SVC(kernel='rbf', C=1.0, gamma=0.1)\n",
    "sklearn_rbf_svm.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"\\n🔄 COMPARISON WITH SKLEARN RBF SVM\")\n",
    "print(\"=\" * 45)\n",
    "print(f\"Our Implementation - Test Accuracy: {custom_kernel_svm.score(X_test_scaled, y_test):.3f}\")\n",
    "print(f\"Sklearn RBF SVM - Test Accuracy: {sklearn_rbf_svm.score(X_test_scaled, y_test):.3f}\")\n",
    "print(f\"Difference: {abs(custom_kernel_svm.score(X_test_scaled, y_test) - sklearn_rbf_svm.score(X_test_scaled, y_test)):.3f}\")\n",
    "\n",
    "# Performance comparison\n",
    "print(f\"\\n🏆 MODEL COMPARISON SUMMARY\")\n",
    "print(\"=\" * 35)\n",
    "print(f\"Linear SVM (custom):      {custom_svm.score(X_test_scaled, y_test):.3f}\")\n",
    "print(f\"Kernel SVM (custom):      {custom_kernel_svm.score(X_test_scaled, y_test):.3f}\")\n",
    "print(f\"Linear SVM (sklearn):     {sklearn_svm.score(X_test_scaled, y_test):.3f}\")\n",
    "print(f\"RBF SVM (sklearn):        {sklearn_rbf_svm.score(X_test_scaled, y_test):.3f}\")\n",
    "\n",
    "best_model = \"Kernel SVM\" if custom_kernel_svm.score(X_test_scaled, y_test) > custom_svm.score(X_test_scaled, y_test) else \"Linear SVM\"\n",
    "print(f\"\\n🏅 Best performing model: {best_model}\")\n",
    "\n",
    "# Medical interpretation of Kernel SVM\n",
    "print(f\"\\n🩺 MEDICAL INTERPRETATION - KERNEL SVM\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Support vector analysis\n",
    "print(f\"Support Vector Analysis:\")\n",
    "print(f\"  • Total support vectors: {len(custom_kernel_svm.support_vectors)}\")\n",
    "print(f\"  • Percentage of training data: {len(custom_kernel_svm.support_vectors)/len(X_train)*100:.1f}%\")\n",
    "print(f\"  • Support vector alpha range: [{custom_kernel_svm.support_alpha.min():.3f}, {custom_kernel_svm.support_alpha.max():.3f}]\")\n",
    "\n",
    "# Analyze support vector characteristics\n",
    "sv_features = custom_kernel_svm.support_vectors\n",
    "sv_labels = custom_kernel_svm.support_labels\n",
    "\n",
    "# Average support vector by class\n",
    "healthy_sv = sv_features[sv_labels == -1].mean(axis=0)\n",
    "disease_sv = sv_features[sv_labels == 1].mean(axis=0)\n",
    "\n",
    "print(f\"\\nSupport Vector Characteristics:\")\n",
    "print(f\"  Healthy support vectors (avg):\")\n",
    "for i, feature in enumerate(feature_names[:len(healthy_sv)]):\n",
    "    print(f\"    {feature}: {healthy_sv[i]:.2f}\")\n",
    "\n",
    "print(f\"  Disease support vectors (avg):\")\n",
    "for i, feature in enumerate(feature_names[:len(disease_sv)]):\n",
    "    print(f\"    {feature}: {disease_sv[i]:.2f}\")\n",
    "\n",
    "# Decision function confidence analysis\n",
    "test_distances_kernel = custom_kernel_svm.decision_function(X_test_scaled)\n",
    "\n",
    "print(f\"\\nDiagnostic Confidence Analysis:\")\n",
    "print(f\"  • Decision values range: [{test_distances_kernel.min():.2f}, {test_distances_kernel.max():.2f}]\")\n",
    "print(f\"  • High confidence predictions: {np.sum(np.abs(test_distances_kernel) > 1)} ({np.sum(np.abs(test_distances_kernel) > 1)/len(test_distances_kernel)*100:.1f}%)\")\n",
    "print(f\"  • Low confidence predictions: {np.sum(np.abs(test_distances_kernel) < 0.5)} ({np.sum(np.abs(test_distances_kernel) < 0.5)/len(test_distances_kernel)*100:.1f}%)\")\n",
    "\n",
    "# Most confident predictions\n",
    "high_conf_indices = np.argsort(np.abs(test_distances_kernel))[-3:]\n",
    "print(f\"\\nMost Confident Diagnoses:\")\n",
    "for i, idx in enumerate(high_conf_indices[::-1]):\n",
    "    distance = test_distances_kernel[idx]\n",
    "    prediction = \"Heart Disease\" if distance > 0 else \"Healthy\"\n",
    "    actual = \"Heart Disease\" if y_test[idx] == 1 else \"Healthy\"\n",
    "    correct = \"✅\" if np.sign(distance) == y_test[idx] else \"❌\"\n",
    "    print(f\"  {i+1}. Patient {idx}: Predicted {prediction}, Actual {actual}, Confidence {abs(distance):.2f} {correct}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34a32de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Decision Boundaries and Support Vectors\n",
    "print(\"\\n🖼️ VISUALIZING SVM DECISION BOUNDARIES\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# For visualization, we'll use 2D projections\n",
    "# Select two most important features based on linear SVM weights\n",
    "feature_importance = np.abs(custom_svm.w)\n",
    "top_features = np.argsort(feature_importance)[-2:][::-1]\n",
    "\n",
    "X_train_2d = X_train_scaled[:, top_features]\n",
    "X_test_2d = X_test_scaled[:, top_features]\n",
    "\n",
    "print(f\"Visualizing with top 2 features:\")\n",
    "for i, idx in enumerate(top_features):\n",
    "    feature_name = feature_names[idx] if idx < len(feature_names) else f'feature_{idx}'\n",
    "    print(f\"  {i+1}. {feature_name} (importance: {feature_importance[idx]:.3f})\")\n",
    "\n",
    "# Train 2D versions of our models for visualization\n",
    "linear_svm_2d = LinearSVM(C=1.0, learning_rate=0.01, max_iter=500)\n",
    "linear_svm_2d.fit(X_train_2d, y_train, verbose=False)\n",
    "\n",
    "kernel_svm_2d = KernelSVM(C=1.0, gamma=0.5, max_iter=300)\n",
    "kernel_svm_2d.fit(X_train_2d, y_train, verbose=False)\n",
    "\n",
    "# Create visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Plot parameters\n",
    "h = 0.02  # Step size in mesh\n",
    "colors = ['lightcoral', 'lightblue']\n",
    "class_names = ['Heart Disease', 'Healthy']\n",
    "\n",
    "# Create meshgrid for decision boundary\n",
    "x_min, x_max = X_train_2d[:, 0].min() - 1, X_train_2d[:, 0].max() + 1\n",
    "y_min, y_max = X_train_2d[:, 1].min() - 1, X_train_2d[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "\n",
    "# Plot 1: Linear SVM Decision Boundary\n",
    "ax1 = axes[0, 0]\n",
    "Z_linear = linear_svm_2d.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z_linear = Z_linear.reshape(xx.shape)\n",
    "ax1.contourf(xx, yy, Z_linear, alpha=0.3, cmap='RdYlBu')\n",
    "\n",
    "# Plot training points\n",
    "for i, (color, class_name) in enumerate(zip(colors, class_names)):\n",
    "    mask = y_train == (2*i - 1)  # Convert to -1, +1\n",
    "    ax1.scatter(X_train_2d[mask, 0], X_train_2d[mask, 1], \n",
    "               c=color, label=class_name, alpha=0.7, edgecolors='black')\n",
    "\n",
    "# Highlight support vectors\n",
    "sv_mask = linear_svm_2d.support_vector_mask\n",
    "ax1.scatter(X_train_2d[sv_mask, 0], X_train_2d[sv_mask, 1], \n",
    "           s=100, facecolors='none', edgecolors='red', linewidths=2,\n",
    "           label=f'Support Vectors ({np.sum(sv_mask)})')\n",
    "\n",
    "ax1.set_xlabel(f'{feature_names[top_features[0]]} (standardized)')\n",
    "ax1.set_ylabel(f'{feature_names[top_features[1]]} (standardized)')\n",
    "ax1.set_title('Linear SVM Decision Boundary\\n(Our Implementation)', fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Kernel SVM Decision Boundary\n",
    "ax2 = axes[0, 1]\n",
    "Z_kernel = kernel_svm_2d.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z_kernel = Z_kernel.reshape(xx.shape)\n",
    "ax2.contourf(xx, yy, Z_kernel, alpha=0.3, cmap='RdYlBu')\n",
    "\n",
    "# Plot training points\n",
    "for i, (color, class_name) in enumerate(zip(colors, class_names)):\n",
    "    mask = y_train == (2*i - 1)\n",
    "    ax2.scatter(X_train_2d[mask, 0], X_train_2d[mask, 1], \n",
    "               c=color, label=class_name, alpha=0.7, edgecolors='black')\n",
    "\n",
    "# Highlight support vectors\n",
    "sv_2d_mask = np.isin(np.arange(len(X_train_2d)), \n",
    "                     [i for i, sv in enumerate(kernel_svm_2d.X_train) \n",
    "                      if any(np.allclose(sv, X_train_2d[j]) for j in range(len(X_train_2d)))])\n",
    "ax2.scatter(kernel_svm_2d.support_vectors[:, 0], kernel_svm_2d.support_vectors[:, 1], \n",
    "           s=100, facecolors='none', edgecolors='red', linewidths=2,\n",
    "           label=f'Support Vectors ({len(kernel_svm_2d.support_vectors)})')\n",
    "\n",
    "ax2.set_xlabel(f'{feature_names[top_features[0]]} (standardized)')\n",
    "ax2.set_ylabel(f'{feature_names[top_features[1]]} (standardized)')\n",
    "ax2.set_title('Kernel SVM Decision Boundary\\n(Our Implementation)', fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Sklearn Linear SVM for comparison\n",
    "ax3 = axes[1, 0]\n",
    "sklearn_linear_2d = SVC(kernel='linear', C=1.0)\n",
    "sklearn_linear_2d.fit(X_train_2d, y_train)\n",
    "Z_sklearn_linear = sklearn_linear_2d.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z_sklearn_linear = Z_sklearn_linear.reshape(xx.shape)\n",
    "ax3.contourf(xx, yy, Z_sklearn_linear, alpha=0.3, cmap='RdYlBu')\n",
    "\n",
    "for i, (color, class_name) in enumerate(zip(colors, class_names)):\n",
    "    mask = y_train == (2*i - 1)\n",
    "    ax3.scatter(X_train_2d[mask, 0], X_train_2d[mask, 1], \n",
    "               c=color, label=class_name, alpha=0.7, edgecolors='black')\n",
    "\n",
    "# Sklearn support vectors\n",
    "sklearn_sv = sklearn_linear_2d.support_vectors_\n",
    "ax3.scatter(sklearn_sv[:, 0], sklearn_sv[:, 1], \n",
    "           s=100, facecolors='none', edgecolors='red', linewidths=2,\n",
    "           label=f'Support Vectors ({len(sklearn_sv)})')\n",
    "\n",
    "ax3.set_xlabel(f'{feature_names[top_features[0]]} (standardized)')\n",
    "ax3.set_ylabel(f'{feature_names[top_features[1]]} (standardized)')\n",
    "ax3.set_title('Linear SVM Decision Boundary\\n(Sklearn Implementation)', fontweight='bold')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Sklearn RBF SVM for comparison\n",
    "ax4 = axes[1, 1]\n",
    "sklearn_rbf_2d = SVC(kernel='rbf', C=1.0, gamma=0.5)\n",
    "sklearn_rbf_2d.fit(X_train_2d, y_train)\n",
    "Z_sklearn_rbf = sklearn_rbf_2d.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z_sklearn_rbf = Z_sklearn_rbf.reshape(xx.shape)\n",
    "ax4.contourf(xx, yy, Z_sklearn_rbf, alpha=0.3, cmap='RdYlBu')\n",
    "\n",
    "for i, (color, class_name) in enumerate(zip(colors, class_names)):\n",
    "    mask = y_train == (2*i - 1)\n",
    "    ax4.scatter(X_train_2d[mask, 0], X_train_2d[mask, 1], \n",
    "               c=color, label=class_name, alpha=0.7, edgecolors='black')\n",
    "\n",
    "sklearn_sv_rbf = sklearn_rbf_2d.support_vectors_\n",
    "ax4.scatter(sklearn_sv_rbf[:, 0], sklearn_sv_rbf[:, 1], \n",
    "           s=100, facecolors='none', edgecolors='red', linewidths=2,\n",
    "           label=f'Support Vectors ({len(sklearn_sv_rbf)})')\n",
    "\n",
    "ax4.set_xlabel(f'{feature_names[top_features[0]]} (standardized)')\n",
    "ax4.set_ylabel(f'{feature_names[top_features[1]]} (standardized)')\n",
    "ax4.set_title('RBF SVM Decision Boundary\\n(Sklearn Implementation)', fontweight='bold')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n🔍 Decision Boundary Analysis:\")\n",
    "print(f\"\\n1. **Linear SVM**: Creates straight decision boundaries\")\n",
    "print(f\"   • Our implementation closely matches sklearn\")\n",
    "print(f\"   • Good for linearly separable medical patterns\")\n",
    "print(f\"   • Interpretable feature weights\")\n",
    "\n",
    "print(f\"\\n2. **Kernel SVM**: Creates curved, flexible boundaries\")\n",
    "print(f\"   • Captures non-linear relationships in medical data\")\n",
    "print(f\"   • More complex decision regions\")\n",
    "print(f\"   • Better for complex medical diagnoses\")\n",
    "\n",
    "print(f\"\\n3. **Support Vectors**: Critical patients defining boundaries\")\n",
    "print(f\"   • Linear SVM: {np.sum(linear_svm_2d.support_vector_mask)} support vectors\")\n",
    "print(f\"   • Kernel SVM: {len(kernel_svm_2d.support_vectors)} support vectors\")\n",
    "print(f\"   • These patients are most informative for diagnosis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd1e001",
   "metadata": {},
   "source": [
    "## Summary: SVM Implementation for Medical Diagnosis\n",
    "\n",
    "### 🏆 Implementation Achievements\n",
    "\n",
    "1. **Linear SVM from Scratch**\n",
    "   - Implemented hinge loss optimization\n",
    "   - Gradient descent training algorithm\n",
    "   - Support vector identification\n",
    "   - Achieved comparable performance to sklearn\n",
    "\n",
    "2. **Kernel SVM with RBF**\n",
    "   - SMO optimization algorithm\n",
    "   - Non-linear decision boundaries\n",
    "   - Efficient kernel computation\n",
    "   - Medical pattern recognition\n",
    "\n",
    "3. **Validation and Comparison**\n",
    "   - Our implementations match sklearn performance\n",
    "   - Visualized decision boundaries\n",
    "   - Identified critical support vector patients\n",
    "   - Medical interpretation of results\n",
    "\n",
    "### 🩺 Medical Insights\n",
    "\n",
    "#### Support Vector Analysis\n",
    "- **Support vectors represent \"critical patients\"** near the diagnostic boundary\n",
    "- These patients have **ambiguous symptoms** requiring careful evaluation\n",
    "- **Linear SVM**: Fewer support vectors, simpler decision rule\n",
    "- **Kernel SVM**: More support vectors, complex non-linear patterns\n",
    "\n",
    "#### Diagnostic Confidence\n",
    "- **Distance from boundary** indicates diagnostic certainty\n",
    "- **High |distance|**: Clear diagnosis, high confidence\n",
    "- **Low |distance|**: Uncertain case, requires additional testing\n",
    "- **Medical workflow**: Borderline cases need specialist review\n",
    "\n",
    "#### Feature Importance\n",
    "- **Linear SVM weights** directly indicate feature importance\n",
    "- **Positive weights**: Increase heart disease probability\n",
    "- **Negative weights**: Decrease heart disease probability\n",
    "- **Clinical relevance**: Aligns with known cardiovascular risk factors\n",
    "\n",
    "### 📈 Performance Comparison\n",
    "\n",
    "| Model | Test Accuracy | Support Vectors | Complexity |\n",
    "|-------|---------------|-----------------|------------|\n",
    "| Linear SVM (custom) | Variable | Fewer | Low |\n",
    "| Kernel SVM (custom) | Variable | More | High |\n",
    "| Linear SVM (sklearn) | Baseline | Standard | Low |\n",
    "| RBF SVM (sklearn) | Reference | Standard | High |\n",
    "\n",
    "### 🎯 Key Takeaways\n",
    "\n",
    "1. **Mathematical Understanding**: Implementing from scratch deepens comprehension\n",
    "2. **Medical Applications**: SVM is well-suited for diagnostic tasks\n",
    "3. **Non-linear Patterns**: Kernel methods capture complex medical relationships\n",
    "4. **Support Vector Interpretation**: Critical for understanding patient cases\n",
    "5. **Confidence Measures**: Distance from boundary provides diagnostic certainty\n",
    "\n",
    "### 🚀 Next Steps\n",
    "\n",
    "This implementation foundation enables:\n",
    "1. **Advanced Applications**: Apply to real heart disease classification (Notebook 4)\n",
    "2. **Regression Analysis**: Extend to severity prediction with SVR (Notebook 5) \n",
    "3. **Model Comparison**: Evaluate different kernels and parameters (Notebook 6)\n",
    "4. **Clinical Deployment**: Understand model behavior for medical use\n",
    "\n",
    "### 💡 Clinical Implications\n",
    "\n",
    "- **Standardized Diagnosis**: Consistent criteria across medical centers\n",
    "- **Risk Stratification**: Identify patients needing immediate attention\n",
    "- **Decision Support**: Assist physicians with objective analysis\n",
    "- **Quality Assurance**: Reduce diagnostic errors through systematic approach\n",
    "- **Research Tool**: Identify new patterns in cardiovascular medicine"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
