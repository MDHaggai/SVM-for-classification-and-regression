{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6e82f76",
   "metadata": {},
   "source": [
    "# SVM Theory and Mathematical Foundations\n",
    "\n",
    "This notebook provides a comprehensive overview of Support Vector Machine (SVM) theory, mathematical foundations, and intuitive explanations.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Introduction to SVMs](#introduction)\n",
    "2. [Mathematical Foundations](#math-foundations)\n",
    "3. [The Kernel Trick](#kernel-trick)\n",
    "4. [Optimization Problem](#optimization)\n",
    "5. [Visual Intuition](#visual-intuition)\n",
    "6. [Practical Considerations](#practical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13972a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "# Import our custom SVM implementations\n",
    "from src.svm.linear_svm import LinearSVM\n",
    "from src.svm.kernel_svm import KernelSVM\n",
    "from src.svm.kernels import *\n",
    "from src.utils.visualization import SVMVisualizer\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Jupyter notebook display settings\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121be61a",
   "metadata": {},
   "source": [
    "## 1. Introduction to SVMs {#introduction}\n",
    "\n",
    "Support Vector Machines (SVMs) are powerful supervised learning algorithms used for both classification and regression tasks. The key idea behind SVMs is to find the optimal hyperplane that separates different classes with the maximum margin.\n",
    "\n",
    "### Key Concepts:\n",
    "- **Hyperplane**: A decision boundary that separates different classes\n",
    "- **Support Vectors**: Data points closest to the hyperplane\n",
    "- **Margin**: Distance between the hyperplane and the nearest data points\n",
    "- **Maximum Margin**: The largest possible margin between classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d71c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple 2D dataset to illustrate SVM concepts\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate linearly separable data\n",
    "class1 = np.random.multivariate_normal([2, 2], [[0.5, 0], [0, 0.5]], 20)\n",
    "class2 = np.random.multivariate_normal([6, 6], [[0.5, 0], [0, 0.5]], 20)\n",
    "\n",
    "X_simple = np.vstack([class1, class2])\n",
    "y_simple = np.hstack([np.ones(20), -np.ones(20)])\n",
    "\n",
    "# Visualize the data\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(class1[:, 0], class1[:, 1], c='red', marker='o', s=100, \n",
    "           alpha=0.7, label='Class +1')\n",
    "plt.scatter(class2[:, 0], class2[:, 1], c='blue', marker='s', s=100, \n",
    "           alpha=0.7, label='Class -1')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Simple Linearly Separable Dataset')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c63056a",
   "metadata": {},
   "source": [
    "## 2. Mathematical Foundations {#math-foundations}\n",
    "\n",
    "### 2.1 Linear SVM Formulation\n",
    "\n",
    "For a binary classification problem, we want to find a hyperplane that separates the data:\n",
    "\n",
    "$$f(x) = w^T x + b$$\n",
    "\n",
    "where:\n",
    "- $w$ is the weight vector (normal to the hyperplane)\n",
    "- $b$ is the bias term\n",
    "- The decision rule is: $\\text{sign}(f(x))$\n",
    "\n",
    "### 2.2 Margin Maximization\n",
    "\n",
    "The margin for a point $(x_i, y_i)$ is:\n",
    "\n",
    "$$\\text{margin}_i = \\frac{y_i(w^T x_i + b)}{||w||}$$\n",
    "\n",
    "We want to maximize the minimum margin:\n",
    "\n",
    "$$\\max_{w,b} \\min_i \\frac{y_i(w^T x_i + b)}{||w||}$$\n",
    "\n",
    "### 2.3 Primal Optimization Problem\n",
    "\n",
    "This leads to the primal optimization problem:\n",
    "\n",
    "$$\\min_{w,b} \\frac{1}{2}||w||^2$$\n",
    "$$\\text{subject to: } y_i(w^T x_i + b) \\geq 1, \\forall i$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc46f51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the concept of margin\n",
    "def plot_svm_margin_concept():\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    \n",
    "    # Create simple data\n",
    "    X = np.array([[1, 2], [2, 3], [3, 3], [6, 5], [7, 6], [8, 7]])\n",
    "    y = np.array([1, 1, 1, -1, -1, -1])\n",
    "    \n",
    "    # Different hyperplanes\n",
    "    hyperplanes = [\n",
    "        {'w': np.array([1, 0]), 'b': -4.5, 'title': 'Poor Separation'},\n",
    "        {'w': np.array([1, 1]), 'b': -6, 'title': 'Better Separation'},\n",
    "        {'w': np.array([0.5, 1]), 'b': -4, 'title': 'Maximum Margin (SVM)'}\n",
    "    ]\n",
    "    \n",
    "    for idx, (ax, hp) in enumerate(zip(axes, hyperplanes)):\n",
    "        # Plot data points\n",
    "        pos_mask = y == 1\n",
    "        neg_mask = y == -1\n",
    "        \n",
    "        ax.scatter(X[pos_mask, 0], X[pos_mask, 1], c='red', marker='o', \n",
    "                  s=100, alpha=0.7, label='Class +1')\n",
    "        ax.scatter(X[neg_mask, 0], X[neg_mask, 1], c='blue', marker='s', \n",
    "                  s=100, alpha=0.7, label='Class -1')\n",
    "        \n",
    "        # Plot hyperplane\n",
    "        x_range = np.linspace(0, 9, 100)\n",
    "        if hp['w'][1] != 0:\n",
    "            y_range = -(hp['w'][0] * x_range + hp['b']) / hp['w'][1]\n",
    "            ax.plot(x_range, y_range, 'k-', linewidth=2, label='Decision Boundary')\n",
    "            \n",
    "            # Plot margin lines for the SVM case\n",
    "            if idx == 2:\n",
    "                y_margin_pos = -(hp['w'][0] * x_range + hp['b'] - 1) / hp['w'][1]\n",
    "                y_margin_neg = -(hp['w'][0] * x_range + hp['b'] + 1) / hp['w'][1]\n",
    "                ax.plot(x_range, y_margin_pos, 'k--', alpha=0.5, label='Margin')\n",
    "                ax.plot(x_range, y_margin_neg, 'k--', alpha=0.5)\n",
    "        \n",
    "        ax.set_xlim(0, 9)\n",
    "        ax.set_ylim(1, 8)\n",
    "        ax.set_xlabel('Feature 1')\n",
    "        ax.set_ylabel('Feature 2')\n",
    "        ax.set_title(hp['title'])\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_svm_margin_concept()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4b2719",
   "metadata": {},
   "source": [
    "### 2.4 Dual Formulation\n",
    "\n",
    "Using Lagrange multipliers, we can derive the dual problem:\n",
    "\n",
    "$$\\max_{\\alpha} \\sum_{i=1}^n \\alpha_i - \\frac{1}{2} \\sum_{i=1}^n \\sum_{j=1}^n \\alpha_i \\alpha_j y_i y_j x_i^T x_j$$\n",
    "\n",
    "$$\\text{subject to: } \\sum_{i=1}^n \\alpha_i y_i = 0, \\quad \\alpha_i \\geq 0$$\n",
    "\n",
    "The optimal weight vector is:\n",
    "$$w^* = \\sum_{i=1}^n \\alpha_i^* y_i x_i$$\n",
    "\n",
    "Points with $\\alpha_i > 0$ are called **support vectors**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0886ec0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a linear SVM and visualize support vectors\n",
    "svm = LinearSVM(C=1.0, max_iter=1000)\n",
    "svm.fit(X_simple, y_simple)\n",
    "\n",
    "# Create visualization\n",
    "visualizer = SVMVisualizer()\n",
    "fig = visualizer.plot_decision_boundary_2d(svm, X_simple, y_simple, \n",
    "                                          title=\"Linear SVM with Support Vectors\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Number of support vectors: {len(svm.support_vectors_)}\")\n",
    "print(f\"Support vector indices: {svm.support_vector_indices_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a17c17",
   "metadata": {},
   "source": [
    "## 3. The Kernel Trick {#kernel-trick}\n",
    "\n",
    "For non-linearly separable data, we can map the input space to a higher-dimensional feature space using a kernel function:\n",
    "\n",
    "$$K(x_i, x_j) = \\phi(x_i)^T \\phi(x_j)$$\n",
    "\n",
    "The dual formulation becomes:\n",
    "$$\\max_{\\alpha} \\sum_{i=1}^n \\alpha_i - \\frac{1}{2} \\sum_{i=1}^n \\sum_{j=1}^n \\alpha_i \\alpha_j y_i y_j K(x_i, x_j)$$\n",
    "\n",
    "### Common Kernel Functions:\n",
    "\n",
    "1. **Linear**: $K(x_i, x_j) = x_i^T x_j$\n",
    "2. **Polynomial**: $K(x_i, x_j) = (\\gamma x_i^T x_j + r)^d$\n",
    "3. **RBF (Gaussian)**: $K(x_i, x_j) = \\exp(-\\gamma ||x_i - x_j||^2)$\n",
    "4. **Sigmoid**: $K(x_i, x_j) = \\tanh(\\gamma x_i^T x_j + r)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fffd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate kernel functions\n",
    "def visualize_kernel_functions():\n",
    "    # Create test points\n",
    "    x = np.linspace(-3, 3, 100)\n",
    "    x_ref = np.array([0])  # Reference point\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    kernels = [\n",
    "        ('Linear', lambda xi: linear_kernel(xi.reshape(-1, 1), x_ref.reshape(-1, 1)).flatten()),\n",
    "        ('Polynomial (d=3)', lambda xi: polynomial_kernel(xi.reshape(-1, 1), x_ref.reshape(-1, 1), degree=3, gamma=1, coef0=1).flatten()),\n",
    "        ('RBF (γ=1)', lambda xi: rbf_kernel(xi.reshape(-1, 1), x_ref.reshape(-1, 1), gamma=1).flatten()),\n",
    "        ('Sigmoid', lambda xi: sigmoid_kernel(xi.reshape(-1, 1), x_ref.reshape(-1, 1), gamma=1, coef0=0).flatten())\n",
    "    ]\n",
    "    \n",
    "    for idx, (name, kernel_func) in enumerate(kernels):\n",
    "        y = kernel_func(x)\n",
    "        axes[idx].plot(x, y, linewidth=3)\n",
    "        axes[idx].axvline(x=0, color='red', linestyle='--', alpha=0.7, label='Reference point')\n",
    "        axes[idx].set_title(f'{name} Kernel')\n",
    "        axes[idx].set_xlabel('Input value')\n",
    "        axes[idx].set_ylabel('Kernel value')\n",
    "        axes[idx].grid(True, alpha=0.3)\n",
    "        axes[idx].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_kernel_functions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9172fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create non-linearly separable data\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate circular data\n",
    "n_samples = 200\n",
    "theta = np.linspace(0, 2*np.pi, n_samples)\n",
    "r1 = 2 + 0.5 * np.random.randn(n_samples//2)\n",
    "r2 = 4 + 0.5 * np.random.randn(n_samples//2)\n",
    "\n",
    "X_circle = np.vstack([\n",
    "    np.column_stack([r1 * np.cos(theta[:n_samples//2]), r1 * np.sin(theta[:n_samples//2])]),\n",
    "    np.column_stack([r2 * np.cos(theta[n_samples//2:]), r2 * np.sin(theta[n_samples//2:])])\n",
    "])\n",
    "y_circle = np.hstack([np.ones(n_samples//2), -np.ones(n_samples//2)])\n",
    "\n",
    "# Compare linear vs RBF kernel\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Linear SVM\n",
    "svm_linear = KernelSVM(kernel='linear', C=1.0)\n",
    "svm_linear.fit(X_circle, y_circle)\n",
    "\n",
    "visualizer.plot_decision_boundary_2d(svm_linear, X_circle, y_circle, \n",
    "                                    title=\"Linear Kernel SVM\", ax=axes[0])\n",
    "\n",
    "# RBF SVM\n",
    "svm_rbf = KernelSVM(kernel='rbf', gamma=0.5, C=1.0)\n",
    "svm_rbf.fit(X_circle, y_circle)\n",
    "\n",
    "visualizer.plot_decision_boundary_2d(svm_rbf, X_circle, y_circle, \n",
    "                                    title=\"RBF Kernel SVM\", ax=axes[1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Linear SVM accuracy: {np.mean(svm_linear.predict(X_circle) == y_circle):.3f}\")\n",
    "print(f\"RBF SVM accuracy: {np.mean(svm_rbf.predict(X_circle) == y_circle):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a72f01",
   "metadata": {},
   "source": [
    "## 4. Optimization Problem {#optimization}\n",
    "\n",
    "### 4.1 Soft Margin SVM\n",
    "\n",
    "For non-separable data, we introduce slack variables $\\xi_i$:\n",
    "\n",
    "$$\\min_{w,b,\\xi} \\frac{1}{2}||w||^2 + C \\sum_{i=1}^n \\xi_i$$\n",
    "$$\\text{subject to: } y_i(w^T x_i + b) \\geq 1 - \\xi_i, \\quad \\xi_i \\geq 0$$\n",
    "\n",
    "The parameter $C$ controls the trade-off between margin maximization and classification error.\n",
    "\n",
    "### 4.2 Sequential Minimal Optimization (SMO)\n",
    "\n",
    "SMO is an efficient algorithm for solving the SVM optimization problem by:\n",
    "1. Selecting two Lagrange multipliers to optimize\n",
    "2. Solving the sub-problem analytically\n",
    "3. Repeating until convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4270df93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate effect of C parameter\n",
    "def demonstrate_c_parameter():\n",
    "    # Create noisy data\n",
    "    np.random.seed(42)\n",
    "    X_noisy = np.vstack([\n",
    "        np.random.multivariate_normal([2, 2], [[1, 0.5], [0.5, 1]], 50),\n",
    "        np.random.multivariate_normal([6, 6], [[1, 0.5], [0.5, 1]], 50)\n",
    "    ])\n",
    "    y_noisy = np.hstack([np.ones(50), -np.ones(50)])\n",
    "    \n",
    "    # Add some noise/outliers\n",
    "    outliers = np.array([[1, 6], [7, 2]])\n",
    "    outlier_labels = np.array([1, -1])\n",
    "    \n",
    "    X_noisy = np.vstack([X_noisy, outliers])\n",
    "    y_noisy = np.hstack([y_noisy, outlier_labels])\n",
    "    \n",
    "    C_values = [0.1, 1.0, 10.0]\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    \n",
    "    for idx, C in enumerate(C_values):\n",
    "        svm = LinearSVM(C=C, max_iter=1000)\n",
    "        svm.fit(X_noisy, y_noisy)\n",
    "        \n",
    "        visualizer.plot_decision_boundary_2d(svm, X_noisy, y_noisy, \n",
    "                                           title=f\"C = {C}\", ax=axes[idx])\n",
    "        \n",
    "        # Highlight outliers\n",
    "        axes[idx].scatter(outliers[:, 0], outliers[:, 1], \n",
    "                         c=['red', 'blue'], marker='x', s=200, linewidths=3,\n",
    "                         label='Outliers')\n",
    "        axes[idx].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "demonstrate_c_parameter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f428e7",
   "metadata": {},
   "source": [
    "## 5. Visual Intuition {#visual-intuition}\n",
    "\n",
    "Let's visualize how SVMs work in different scenarios and understand the geometric intuition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c9a601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D visualization of kernel transformation\n",
    "def visualize_kernel_transformation():\n",
    "    # Create 2D data that's not linearly separable\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Create XOR-like data\n",
    "    X_2d = np.array([\n",
    "        [1, 1], [1, -1], [-1, 1], [-1, -1],  # Core points\n",
    "        [0.8, 0.8], [0.8, -0.8], [-0.8, 0.8], [-0.8, -0.8],  # Close points\n",
    "        [1.2, 1.2], [1.2, -1.2], [-1.2, 1.2], [-1.2, -1.2],  # Far points\n",
    "    ])\n",
    "    y_2d = np.array([1, -1, -1, 1, 1, -1, -1, 1, 1, -1, -1, 1])\n",
    "    \n",
    "    # Transform to 3D using polynomial features (x1, x2, x1*x2)\n",
    "    X_3d = np.column_stack([\n",
    "        X_2d[:, 0],\n",
    "        X_2d[:, 1], \n",
    "        X_2d[:, 0] * X_2d[:, 1]\n",
    "    ])\n",
    "    \n",
    "    fig = plt.figure(figsize=(16, 6))\n",
    "    \n",
    "    # 2D plot\n",
    "    ax1 = fig.add_subplot(121)\n",
    "    colors = ['red' if label == 1 else 'blue' for label in y_2d]\n",
    "    ax1.scatter(X_2d[:, 0], X_2d[:, 1], c=colors, s=100, alpha=0.7)\n",
    "    ax1.set_xlabel('X1')\n",
    "    ax1.set_ylabel('X2')\n",
    "    ax1.set_title('Original 2D Space (Not Linearly Separable)')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3D plot\n",
    "    ax2 = fig.add_subplot(122, projection='3d')\n",
    "    ax2.scatter(X_3d[:, 0], X_3d[:, 1], X_3d[:, 2], c=colors, s=100, alpha=0.7)\n",
    "    ax2.set_xlabel('X1')\n",
    "    ax2.set_ylabel('X2')\n",
    "    ax2.set_zlabel('X1 * X2')\n",
    "    ax2.set_title('Transformed 3D Space (Linearly Separable)')\n",
    "    \n",
    "    # Add separating plane in 3D\n",
    "    xx, yy = np.meshgrid(np.linspace(-2, 2, 10), np.linspace(-2, 2, 10))\n",
    "    zz = np.zeros_like(xx)  # Plane at z=0 separates the classes\n",
    "    ax2.plot_surface(xx, yy, zz, alpha=0.3, color='green')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_kernel_transformation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63393328",
   "metadata": {},
   "source": [
    "## 6. Practical Considerations {#practical}\n",
    "\n",
    "### 6.1 Hyperparameter Selection\n",
    "\n",
    "Key hyperparameters to tune:\n",
    "- **C**: Regularization parameter (higher C = less regularization)\n",
    "- **γ (gamma)**: RBF kernel parameter (higher γ = more complex boundary)\n",
    "- **Kernel choice**: Linear, polynomial, RBF, sigmoid\n",
    "\n",
    "### 6.2 Preprocessing\n",
    "\n",
    "- **Feature scaling**: SVMs are sensitive to feature scales\n",
    "- **Feature selection**: Remove irrelevant features\n",
    "- **Handle missing values**: Imputation strategies\n",
    "\n",
    "### 6.3 Model Selection\n",
    "\n",
    "Use cross-validation to select:\n",
    "- Best kernel function\n",
    "- Optimal hyperparameters\n",
    "- Model complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7e658b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter sensitivity analysis\n",
    "def hyperparameter_sensitivity():\n",
    "    from sklearn.model_selection import validation_curve\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn.datasets import make_classification\n",
    "    \n",
    "    # Generate dataset\n",
    "    X, y = make_classification(n_samples=200, n_features=2, n_redundant=0, \n",
    "                              n_informative=2, n_clusters_per_class=1, \n",
    "                              random_state=42)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # C parameter sensitivity\n",
    "    C_range = np.logspace(-3, 3, 7)\n",
    "    train_scores, test_scores = validation_curve(\n",
    "        SVC(kernel='rbf', gamma='scale'), X, y, param_name='C', \n",
    "        param_range=C_range, cv=5, scoring='accuracy'\n",
    "    )\n",
    "    \n",
    "    train_mean = np.mean(train_scores, axis=1)\n",
    "    train_std = np.std(train_scores, axis=1)\n",
    "    test_mean = np.mean(test_scores, axis=1)\n",
    "    test_std = np.std(test_scores, axis=1)\n",
    "    \n",
    "    axes[0].semilogx(C_range, train_mean, 'o-', color='blue', label='Training')\n",
    "    axes[0].fill_between(C_range, train_mean - train_std, train_mean + train_std, alpha=0.1, color='blue')\n",
    "    axes[0].semilogx(C_range, test_mean, 'o-', color='red', label='Validation')\n",
    "    axes[0].fill_between(C_range, test_mean - test_std, test_mean + test_std, alpha=0.1, color='red')\n",
    "    axes[0].set_xlabel('C Parameter')\n",
    "    axes[0].set_ylabel('Accuracy')\n",
    "    axes[0].set_title('C Parameter Sensitivity')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Gamma parameter sensitivity\n",
    "    gamma_range = np.logspace(-4, 1, 6)\n",
    "    train_scores, test_scores = validation_curve(\n",
    "        SVC(kernel='rbf', C=1.0), X, y, param_name='gamma', \n",
    "        param_range=gamma_range, cv=5, scoring='accuracy'\n",
    "    )\n",
    "    \n",
    "    train_mean = np.mean(train_scores, axis=1)\n",
    "    train_std = np.std(train_scores, axis=1)\n",
    "    test_mean = np.mean(test_scores, axis=1)\n",
    "    test_std = np.std(test_scores, axis=1)\n",
    "    \n",
    "    axes[1].semilogx(gamma_range, train_mean, 'o-', color='blue', label='Training')\n",
    "    axes[1].fill_between(gamma_range, train_mean - train_std, train_mean + train_std, alpha=0.1, color='blue')\n",
    "    axes[1].semilogx(gamma_range, test_mean, 'o-', color='red', label='Validation')\n",
    "    axes[1].fill_between(gamma_range, test_mean - test_std, test_mean + test_std, alpha=0.1, color='red')\n",
    "    axes[1].set_xlabel('Gamma Parameter')\n",
    "    axes[1].set_ylabel('Accuracy')\n",
    "    axes[1].set_title('Gamma Parameter Sensitivity')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "hyperparameter_sensitivity()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495331f0",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Key takeaways from SVM theory:\n",
    "\n",
    "1. **Geometric Intuition**: SVMs find the hyperplane with maximum margin\n",
    "2. **Mathematical Foundation**: Based on convex optimization theory\n",
    "3. **Kernel Trick**: Enables non-linear classification without explicit feature mapping\n",
    "4. **Support Vectors**: Only a subset of training points determine the model\n",
    "5. **Regularization**: C parameter controls overfitting\n",
    "6. **Preprocessing**: Feature scaling is crucial for SVM performance\n",
    "\n",
    "### When to Use SVMs:\n",
    "- ✅ High-dimensional data\n",
    "- ✅ Clear margin of separation\n",
    "- ✅ Small to medium datasets\n",
    "- ✅ Non-linear relationships (with kernels)\n",
    "\n",
    "### When NOT to Use SVMs:\n",
    "- ❌ Very large datasets (slow training)\n",
    "- ❌ Noisy data with overlapping classes\n",
    "- ❌ Need probability estimates (requires calibration)\n",
    "- ❌ Many irrelevant features"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
